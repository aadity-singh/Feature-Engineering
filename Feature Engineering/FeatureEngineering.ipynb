{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8203a64-e7c1-43c7-98a2-a47512b2941e",
   "metadata": {},
   "source": [
    "# THEORY QUESTIONS:-"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb864e09-cc6d-47ee-8b55-66936ec69aea",
   "metadata": {},
   "source": [
    "# 1-What is a parameter?\n",
    " -> In machine learning (ML), a parameter is a variable that a model learns from the data during training. Parameters define the\n",
    "    behavior of the model and directly influence its predictions.These parameters are adjusted through training using techniques like gradient     descent to minimize errors and improve accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277f750e-975e-4171-a94a-d7f1c90ad209",
   "metadata": {},
   "source": [
    "# 2-What is correlation? What does negative correlation mean?\n",
    "-> Correlation is a statistical measure that describes how two variables move in relation to each other. It helps identify whether an \n",
    "   increase or decrease in one variable is associated with an increase or decrease in another.Correlation values range from -1 to +1.\n",
    "   +1: Perfect positive correlation.\n",
    "   Negative correlation: When one variable increases, the other tends to decrease. For example, the more time spent exercising, the lower a       person's body fat percentage tends to be.-1: Perfect negative correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11261e1-2f30-43b9-8300-805358d3594d",
   "metadata": {},
   "source": [
    "# 3- Define Machine Learning. What are the main components in Machine Learning?\n",
    "  -> Machine Learning is a branch of artificial intelligence that enables systems to learn patterns from data and make predictions or              decisions without being explicitly programmed. ML algorithms improve their performance over time by analyzing data and adjusting their        parameters.\n",
    "     Main Components in Machine Learning\n",
    "     1- Data – The foundation of ML; structured or unstructured information used to train models.   \n",
    "     2- Features – Relevant attributes extracted from data to help the model understand patterns.  \n",
    "     3- Model – A mathematical representation of learned patterns used for predictions.\n",
    "     4- Training – The process where the model learns by adjusting its parameters based on data.   \n",
    "     5- Evaluation – Assessing the model’s performance using metrics like accuracy, precision, and recall.   \n",
    "     6- Deployment – Using the trained model in real-world applications to make decisions.  \n",
    "     7- Optimization – Fine-tuning hyperparameters and improving the model’s performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "609fe3e2-d1a6-40ee-b2e2-05e9679cd4b7",
   "metadata": {},
   "source": [
    "# 4- How does loss value help in determining whether the model is good or not?\n",
    "  -> Loss value is a key metric in machine learning that indicates how well a model is performing. It measures the difference between the          model's predictions and the actual target values. Here's how it helps assess the quality of a model:\n",
    "     How Loss Value Determines Model Quality\n",
    "     1- Lower Loss = Better Model: A lower loss value means the model's predictions are closer to the actual values, indicating good                               performance.  \n",
    "     2- High Loss = Poor Model Performance: A high loss suggests the model is making inaccurate predictions, meaning it needs improvement.\n",
    "     3- Tracking Training Progress: Loss is used to monitor how well the model is learning during training. If loss decreases over time, the                                      model is improving. \n",
    "     4- Overfitting vs. Underfitting:  \n",
    "        If training loss is low but validation loss is high, the model may be overfitting (memorizing data but failing on new inputs).\n",
    "        If both losses are high, the model is underfitting, meaning it's not learning useful patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6aa28a7-f912-4ab2-8741-73db19a60489",
   "metadata": {},
   "source": [
    "# 5- What are continuous and categorical variables?\n",
    "  -> Continuous Variables:-\n",
    "    These variables can take any numerical value within a range.They are measurable and often represented by real numbers.  \n",
    "    Examples: Height (e.g., 170.5 cm)  \n",
    "              Temperature (e.g., 36.7°C)   \n",
    "              Weight (e.g., 65.2 kg)   \n",
    "    Continuous data can be further divided into interval and ratio variables.\n",
    "    Categorical Variables:-\n",
    "    These variables represent distinct categories or groups.  \n",
    "    They do not have a numerical relationship or order (unless they are ordinal). \n",
    "    Examples:  Gender (Male, Female, Other)   \n",
    "               Type of car (Sedan, SUV, Truck) \n",
    "               Blood group (A, B, AB, O)    \n",
    "    Categorical variables can be nominal (no order, like colors) or ordinal (with order, like education levels)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad7b90f-7697-4232-a069-489a4cd73e67",
   "metadata": {},
   "source": [
    "# 6- How do we handle categorical variables in Machine Learning? What are the common techniques?\n",
    "   -> Handling categorical variables in machine learning is crucial since most ML algorithms work better with numerical data. Here are some         common techniques to transform categorical data into a usable format:\n",
    "      1. Encoding Techniques\n",
    "      2. Hashing & Embedding\n",
    "      3. Frequency-Based Techniques\n",
    "      4. Handling Rare Categories     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb9f0e96-26b0-423a-a5a0-ab2de60707a1",
   "metadata": {},
   "source": [
    "# 7- What do you mean by training and testing a dataset?\n",
    "  -> In machine learning, training and testing datasets are essential for building and evaluating a model.\n",
    "     Training Dataset:-\n",
    "         This is the dataset used to teach the model.    \n",
    "         The model learns patterns by adjusting its parameters based on this data.    \n",
    "         The goal is to minimize the error by improving predictions. \n",
    "         Example: If you're training a model to classify animals, you'd provide labeled images of cats, dogs, etc.\n",
    "     Testing Dataset:-\n",
    "        This is separate from the training data and is used to evaluate the model’s performance.\n",
    "        The model makes predictions on this data to check accuracy.\n",
    "        It helps assess if the model has generalized well to unseen data.\n",
    "        Example: After training an animal classifier, you'd test it on new, unseen images."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e33771-a507-4534-87c9-a316c52547ae",
   "metadata": {},
   "source": [
    "# 8-  What is sklearn.preprocessing?\n",
    "   -> sklearn.preprocessing is a module in scikit-learn, a popular Python library for machine learning. It provides various tools to                transform raw data into a format suitable for ML models. Preprocessing is crucial because many algorithms perform better when data is         normalized, scaled, or encoded properly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83dc5a-d666-4eea-966c-07b2517e9c01",
   "metadata": {},
   "source": [
    "# 9-  What is a Test set?\n",
    "  -> A test set is a portion of a dataset used to evaluate a machine learning model after it has been trained. It helps measure the model's        ability to generalize to new, unseen data. It is used to ensures the model performs well on data it hasn’t seen before.\n",
    "     Prevents overfitting, where a model learns training data too well but struggles with new inputs.Provides an unbiased estimate of model        accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6433f20b-1303-42d2-b414-2ec939702b05",
   "metadata": {},
   "source": [
    "# 10-  How do we split data for model fitting (training and testing) in Python?How do you approach a Machine Learning problem?\n",
    "  ->  Splitting Data for Model Training & Testing in Python:-\n",
    "      In machine learning, we split data into training and testing sets to evaluate model performance. This can be done using scikit-learn:\n",
    "      Using train_test_split()\n",
    "      test_size=0.2 → 20% of data is allocated for testing.\n",
    "      random_state=42 → Ensures reproducibility.\n",
    "        Approach to a Machine Learning Problem:-\n",
    "        Solving an ML problem requires a structured approach. Here’s a roadmap:       \n",
    "        Step 1: Define the Problem\n",
    "                Understand the goal (classification, regression, clustering, etc.).Identify the target variable and features. \n",
    "        Step 2: Collect & Prepare Data\n",
    "                Gather relevant data. Handle missing values, remove duplicates. Perform EDA (Exploratory Data Analysis) to understand                         patterns.       \n",
    "        Step 3: Feature Engineering\n",
    "                Encode categorical variables.Normalize/scale numerical features.Perform dimensionality reduction if needed.\n",
    "        Step 4: Select a Model\n",
    "                Choose suitable ML algorithms (e.g., linear regression, decision trees, neural networks).Consider complexity and                              computational cost.       \n",
    "        Step 5: Train the Model\n",
    "                Split data into training & testing sets. Train the model using supervised or unsupervised learning.Optimize hyperparameters                   using techniques like GridSearchCV.        \n",
    "        Step 6: Evaluate Model Performance\n",
    "                Use metrics like accuracy, precision, recall, F1-score for classification.Use MSE, RMSE, R² for regression.Ensure the model                   generalizes well without overfitting.\n",
    "        Step 7: Improve the Model\n",
    "                Tune hyperparameters.Use techniques like regularization and feature selection. Experiment with different algorithms.        \n",
    "        Step 8: Deploy the Model\n",
    "                Save the trained model (joblib, pickle).Use Flask or FastAPI for deployment. Monitor model performance over time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f1641da-4a34-4cd5-92e0-c3b6a6b2828f",
   "metadata": {},
   "source": [
    "# 11- Why do we have to perform EDA before fitting a model to the data?\n",
    "  -> Performing Exploratory Data Analysis (EDA) before fitting a model is crucial because it helps us understand the dataset, detect issues,       and make informed decisions that lead to better model performance.\n",
    "     EDA is important for:-\n",
    "     1- Identifies Missing Values – Helps check for and handle missing data using techniques like imputation.\n",
    "     2- Detects Outliers – Outliers can distort model accuracy, so EDA helps decide whether to remove or transform them.  \n",
    "     3- Feature Relationships – Helps visualize correlations between features to select the most relevant ones.   \n",
    "     4- Distribution of Data – Understanding how data is distributed (normal, skewed, etc.) helps choose the right scaling techniques.    \n",
    "     5- Checks Data Quality – Reveals inconsistencies, duplicate entries, and anomalies in the dataset.    \n",
    "     6- Feature Selection & Engineering – Helps identify unnecessary variables and create new useful features.   \n",
    "     7- Guides Model Selection – Understanding data characteristics can help decide whether to use linear models, tree-based models, or deep                                  learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f91f33c-f921-4579-ac36-70c0541db46a",
   "metadata": {},
   "source": [
    "# 12- What is correlation?\n",
    "  -> Correlation is a statistical measure that describes how two variables are related to each other. It helps determine whether an increase       or decrease in one variable is associated with an increase or decrease in another.\n",
    "    Types of Correlation\n",
    "    Positive Correlation – When one variable increases, the other tends to increase as well.    \n",
    "                           Example: The more hours you study, the higher your exam scores.    \n",
    "    Negative Correlation – When one variable increases, the other tends to decrease.   \n",
    "                           Example: The more time spent exercising, the lower body fat percentage tends to be.   \n",
    "    Zero Correlation –     No relationship between the two variables.    \n",
    "                           Example: Hair color and intelligence level."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f274a163-3e3a-4e6f-9465-28fcef93cd51",
   "metadata": {},
   "source": [
    "# 13- What does negative correlation mean?\n",
    "  -> Negative correlation means that as one variable increases, the other decreases. In other words, they move in opposite directions.\n",
    "     Negative Correlation Coefficient:-\n",
    "     Ranges between -1 and 0:\n",
    "    -1: Perfect negative correlation (one variable increases, the other always decreases).    \n",
    "    -0.5: Moderate negative correlation.    \n",
    "     0: No correlation at all."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88451087-5663-4a38-a31e-6dab6c20b9c5",
   "metadata": {},
   "source": [
    "# 14- How can you find correlation between variables in Python?\n",
    "  ->  You can find correlation between variables in Python using pandas or NumPy. The most common method is Pearson correlation, but other          types include Spearman and Kendall correlation.Using pandas: corr() method,this computes the correlation between all numerical columns.\n",
    "      Using NumPy: corrcoef()\n",
    "      Correlation Methods:-\n",
    "        Pearson (default) – Measures linear correlation.\n",
    "        Spearman – Measures monotonic relationships (rank-based correlation).\n",
    "        Kendall – Measures ordinal association."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcf8da8-5570-40bd-82ae-a9cc39101fe7",
   "metadata": {},
   "source": [
    "#15- What is causation? Explain difference between correlation and causation with an example.\n",
    "    -> Causation means that one event directly causes another to happen. In other words, a change in one variable produces a change in               another. \n",
    "       Difference Between Correlation and Causation:-\n",
    "        Correlation shows a relationship between two variables, but it does NOT imply that one causes the other.\n",
    "        Causation confirms that one variable is the direct reason for the change in another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db849dcd-b475-4b7a-9c41-08bce4a91991",
   "metadata": {},
   "source": [
    "# 16-  What is an Optimizer? What are different types of optimizers? Explain each with an example.\n",
    "  -> An optimizer is an algorithm that adjusts the parameters (weights and biases) of a machine learning model to minimize the loss function.      The goal of optimization is to improve the model’s performance by finding the best set of parameters that minimize errors.\n",
    "     Types of Optimizers:-\n",
    "     1. Gradient Descent:-One of the simplest optimization algorithms that minimizes the loss function by updating parameters based on the                              gradient.\n",
    "                          from tensorflow.keras.optimizers import SGD\n",
    "                          optimizer = SGD(learning_rate=0.01)\n",
    "     2. Adam (Adaptive Moment Estimation):-Combines Momentum and RMSprop to achieve faster convergence and adaptability.\n",
    "                                          from tensorflow.keras.optimizers import Adam\n",
    "                                          optimizer = Adam(learning_rate=0.001)\n",
    "     3. RMSprop (Root Mean Square Propagation):-Controls the learning rate dynamically by dividing it by the moving average of squared                                                        gradients.\n",
    "                                              from tensorflow.keras.optimizers import RMSprop\n",
    "                                              optimizer = RMSprop(learning_rate=0.001)\n",
    "     4. AdaGrad (Adaptive Gradient Algorithm):-Adjusts learning rates for each parameter independently based on past gradients.\n",
    "                                               from tensorflow.keras.optimizers import Adagrad\n",
    "                                               optimizer = Adagrad(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2be7b0-ddb4-45d1-931f-7fc1e9a0bb1a",
   "metadata": {},
   "source": [
    "#17- What is sklearn.linear_model ?\n",
    "  -> sklearn.linear_model is a module in scikit-learn, a powerful machine learning library in Python. It provides various linear models used       for regression and classification tasks. \n",
    "     Common Models in sklearn.linear_model:-\n",
    "     1-Linear Regression – Fits a linear relationship between input features and the target variable.\n",
    "     2-Logistic Regression – Used for classification problems (binary or multiclass).\n",
    "     3-Ridge Regression (Ridge) and Lasso Regression (Lasso) – Variants of linear regression with regularization to prevent overfitting.\n",
    "     4-SGD Classifier & Regressor (SGDClassifier, SGDRegressor) – Implements stochastic gradient descent for efficient learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd24b552-a8af-4e15-9901-5eaea9886de9",
   "metadata": {},
   "source": [
    "#18-What does model.fit() do? What arguments must be given?\n",
    " -> The .fit() method in scikit-learn is used to train a machine learning model. When you call model.fit(X, y), the model learns patterns         from the input data X and corresponding labels y. It estimates the parameters of the model based on the given dataset.\n",
    "    Arguments:\n",
    "    X: The feature matrix (independent variables). It's usually a NumPy array or Pandas DataFrame, shaped as (n_samples, n_features).\n",
    "    y: The target variable (dependent variable). For regression, it's a continuous vector; for classification, it's class labels.\n",
    "    Additional arguments may be available depending on the specific model, such as:\n",
    "    sample_weight: Optional weights for individual samples.\n",
    "    epochs, batch_size: If using neural networks or certain iterative algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc61a86e-2458-4a70-b1e8-b03895c85137",
   "metadata": {},
   "source": [
    "#19- What does model.predict() do? What arguments must be given?\n",
    "  -> The .predict() method in scikit-learn is used to make predictions using a trained model. After fitting the model with .fit(X, y),             calling model.predict(X_new) applies the learned patterns to new input data.\n",
    "     Arguments:\n",
    "     X_new: The feature matrix for which predictions are needed. It should have the same number of features as the training data.\n",
    "     Some models may have additional optional parameters, like thresholds (for probabilistic models) or sample_weight."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948119a2-f316-40d4-8a82-537c6671c8d3",
   "metadata": {},
   "source": [
    "#20- What are continuous and categorical variables?\n",
    "-> Continuous Variables:\n",
    "    These take numerical values and can be measured on a continuous scale.\n",
    "    They have an infinite range of possible values within a given interval.    \n",
    "    Examples: Age, height, weight, temperature, salary, or time.   \n",
    "    Categorical Variables:\n",
    "    These represent distinct categories or labels rather than numerical values.\n",
    "    They are often non-numeric, but when numeric, they don’t have meaningful mathematical properties (e.g., ZIP codes).    \n",
    "    Examples: Gender (Male/Female), Blood Type (A, B, AB, O), Car Brand (Toyota, Ford, BMW), or Education Level (High School, Bachelor’s,                   Master’s).   \n",
    "    Categorical variables can be further divided into:   \n",
    "    Nominal Variables: Categories have no inherent order (e.g., Car Brands).    \n",
    "    Ordinal Variables: Categories have a logical order (e.g., Education Level)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb942fab-398b-4fab-beed-2f8bb2f93d69",
   "metadata": {},
   "source": [
    "#21-What is feature scaling? How does it help in Machine Learning?\n",
    " -> Feature scaling is a technique in machine learning that adjusts the range of numerical features so they have comparable scales. This          ensures that all features contribute equally to a model’s learning process, preventing bias toward features with larger values.\n",
    "    Improves Model Performance: Some algorithms (like gradient-based methods) perform better when features are in similar ranges.\n",
    "    Speeds Up Training: Standardized features help models converge faster by reducing computational complexity.\n",
    "    Prevents Dominance: Features with larger numerical values won’t unfairly influence the model’s learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "880ab0e5-557f-4b96-8e40-b53665bd6eae",
   "metadata": {},
   "source": [
    "#22-How do we perform scaling in Python?\n",
    "  -> In Python, we perform feature scaling using libraries like scikit-learn. The most common methods are Standardization (Z-score scaling)        and Normalization (Min-Max scaling). Here's how to do both:\n",
    "     1. Standardization (Z-score Scaling):-Centers data around 0 with a standard deviation of 1.\n",
    "        Example:- from sklearn.preprocessing import StandardScaler\n",
    "                    import numpy as np   \n",
    "                    # Sample data\n",
    "                    X = np.array([[100, 2], [200, 3], [300, 4]])                   \n",
    "                    # Apply standardization\n",
    "                    scaler = StandardScaler()\n",
    "                    X_scaled = scaler.fit_transform(X)                    \n",
    "                    print(X_scaled)\n",
    "     2.Normalization (Min-Max Scaling):-Rescales values between 0 and 1.\n",
    "       Example: from sklearn.preprocessing import MinMaxScaler\n",
    "                # Apply Min-Max scaling\n",
    "                scaler = MinMaxScaler()\n",
    "                X_scaled = scaler.fit_transform(X)                \n",
    "                print(X_scaled)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a593af8-22e3-4239-815b-dbe519a5f9e5",
   "metadata": {},
   "source": [
    "#23- What is sklearn.preprocessing?\n",
    "  -> sklearn.preprocessing is a module in scikit-learn that provides various functions to prepare and transform data before feeding it into a      machine learning model. Proper preprocessing ensures that models learn effectively and produce accurate results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dda5d5-abf1-434a-b1b7-4ef7f40c122d",
   "metadata": {},
   "source": [
    "#24-How do we split data for model fitting (training and testing) in Python?\n",
    "  -> In Python, we use train_test_split from scikit-learn to divide our dataset into training and testing sets. The training set is used to        train the model, while the testing set evaluates its performance.\n",
    "     Example:   from sklearn.model_selection import train_test_split\n",
    "                import numpy as np\n",
    "                # Sample dataset (features and labels)\n",
    "                X = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]])\n",
    "                y = np.array([0, 1, 0, 1, 0])  # Labels\n",
    "                # Splitting the data (80% training, 20% testing)\n",
    "                X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "                # Output the split\n",
    "                print(\"Training Features:\", X_train)\n",
    "                print(\"Testing Features:\", X_test)\n",
    "                print(\"Training Labels:\", y_train)\n",
    "                print(\"Testing Labels:\", y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e86f99a-0772-44fd-af07-0588b64ac869",
   "metadata": {},
   "source": [
    "#25- Explain data encoding?\n",
    "  -> Data encoding is the process of converting categorical or text-based data into numerical format so that machine learning models can           process it effectively. Since models work with numbers, encoding is a crucial preprocessing step.\n",
    "     1️- Label Encoding:Converts categorical variables into integer values.\n",
    "     2- One-Hot Encoding:Converts categories into binary columns (1s and 0s).\n",
    "     3- Ordinal Encoding:Assigns numeric values based on order/rank.\n",
    "     4- Binary Encoding:Represents categories in binary format (more memory-efficient)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19e4b866-1bad-4198-9a90-455b7b05f0e4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
